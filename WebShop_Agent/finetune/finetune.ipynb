{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth \"xformers==0.0.28.post2\"\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import os\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "import torch\n",
    "import random\n",
    "import dotenv\n",
    "from __future__ import print_function, division\n",
    "from typing import List, Dict, Any, Optional, Callable, Tuple\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import argparse\n",
    "from pydantic import BaseModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a shopping agent shopping in Webshop.\n",
    "\n",
    "The actions available to you are :\n",
    "- reset\n",
    "- think[Thought]\n",
    "- search[Search query]\n",
    "- click[Button to click]\n",
    "\n",
    "Rules:\n",
    "- You can reset from any page, think on any page.\n",
    "- You can only click buttons available on the page described in the observation. Buttons are defined between square brackets - []\n",
    "- You can only search from a page with [Search], so click on the back buttons to reach such a page before you search again.\n",
    "- You can ONLY reply with the action you want to take.\n",
    "- You must end after a few tries by attempting to buy something.\n",
    "\n",
    "Tips:\n",
    "- Carefully surf the webshop to fullfil requirements. \n",
    "- If any items match some of the requirements, click on them to see a detailed description and to see if they match all the requirements.  Quantity requirements can be met \n",
    "- Don't just give up on a search at the 1st page of results. Move through the result pages by pressing the [Next >] button. You may decide to give up at a reasonable point such as when the results are empty or too different from the requirements (usually 2-3 pages).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    input_text = prompt + row['instruction'] # concat prompt and requirement\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": 'You are a shopping agent for Webshop, a text-based e-commerce platform.'},\n",
    "        {\"role\": \"user\", \"content\": input_text},\n",
    "        {\"role\": \"assistant\", \"content\": row['trajectory']}\n",
    "    ]\n",
    "\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(conversation, tokenize=False) # applies unsloth function to convert conversation to templated format\n",
    "  \n",
    "    return row\n",
    "\n",
    "processed_data = list(map(format_chat_template, train_data))\n",
    "train_dataset = Dataset.from_list(processed_data)\n",
    "\n",
    "processed_data = list(map(format_chat_template, test_data))\n",
    "test_dataset = Dataset.from_list(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need function to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        # warmup_steps = 5,\n",
    "        num_train_epochs = 5,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 100,\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 1e-5,\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
